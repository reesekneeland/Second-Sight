{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torchvision.transforms as T\n",
    "from data_utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import PIL\n",
    "os.chdir(\"..\")\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7253e-09) tensor(1.0000)\n",
      "tensor(3.4769e-08) tensor(1.0000)\n",
      "tensor(4.9671e-08) tensor(1.0000)\n",
      "tensor(1.1797e-08) tensor(1.0000)\n",
      "tensor(-3.7253e-09) tensor(1.0000)\n",
      "tensor(-6.2088e-09) tensor(1.0000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/naxos2-raid25/kneel027/home/kneel027/Second-Sight/data/mental_imagery.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhypatia.cmrr.umn.edu/home/naxos2-raid25/kneel027/home/kneel027/Second-Sight/data/mental_imagery.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m subject \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m7\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhypatia.cmrr.umn.edu/home/naxos2-raid25/kneel027/home/kneel027/Second-Sight/data/mental_imagery.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# create_whole_region_imagery_unnormalized(subject, mask=False)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhypatia.cmrr.umn.edu/home/naxos2-raid25/kneel027/home/kneel027/Second-Sight/data/mental_imagery.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     create_whole_region_imagery_normalized(subject, mask\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/Second-Sight/data/data_utils.py:197\u001b[0m, in \u001b[0;36mcreate_whole_region_imagery_normalized\u001b[0;34m(subject, mask)\u001b[0m\n\u001b[1;32m    195\u001b[0m     whole_region \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mdata/preprocessed_data/subject\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/nsd_imagery_unnormalized.pt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject))\n\u001b[1;32m    196\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     whole_region \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mdata/preprocessed_data/subject\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m/nsd_imagery_unnormalized_unmasked.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(subject))\n\u001b[1;32m    198\u001b[0m whole_region \u001b[39m=\u001b[39m whole_region \u001b[39m/\u001b[39m \u001b[39m300.\u001b[39m\n\u001b[1;32m    199\u001b[0m whole_region_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(whole_region)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/miniconda3/envs/SS/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/miniconda3/envs/SS/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/miniconda3/envs/SS/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/miniconda3/envs/SS/lib/python3.10/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for subject in [1,2,5,7]:\n",
    "    # create_whole_region_imagery_unnormalized(subject, mask=False)\n",
    "    create_whole_region_imagery_normalized(subject, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x, mean=None, stddev=None, return_stats=False):\n",
    "    new_x = x\n",
    "    if mean is not None:\n",
    "        m = mean\n",
    "    else:\n",
    "        m = torch.mean(new_x, axis=0, keepdims=True)\n",
    "    if stddev is not None:\n",
    "        s = stddev\n",
    "    else:\n",
    "        s = torch.std(new_x, axis=0, keepdims=True)\n",
    "    if return_stats:\n",
    "        return (x - m)/(s+1e-6), m, s\n",
    "    else:\n",
    "        x = torch.where(s==0, (new_x - m), (new_x - m)/s)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['meta_dm', 'gdm', 'gcm', 'image_data', 'image_map', 'exps', 'cues', 'modes'])\n"
     ]
    }
   ],
   "source": [
    "# nsd_root = '/export/raid1/home/surly/raid1/kendrick-data/nsd/'\n",
    "# stim_dir = nsd_root + 'nsddata_stimuli/stimuli/nsd/'\n",
    "# beta_dir = nsd_root + 'nsddata_betas/ppdata/'\n",
    "# mask_dir= nsd_root + 'nsddata/ppdata/'\n",
    "# img_stim_file = stim_dir + \"nsdimagery_stimuli.pkl3\"\n",
    "# beta_subj = beta_dir + \"subj%02d/func1pt8mm/nsdimagerybetas_fithrf/betas_nsdimagery.nii.gz\"%subj\n",
    "\n",
    "def image_feature_fn(image):\n",
    "    '''take uint8 image and return floating point (0,1), either color or bw'''\n",
    "    return image.astype(np.float32) / 255\n",
    "\n",
    "## LOAD THE STIM IMAGES AND SEQUENCE ALIGNMENT DESCRIPTORS\n",
    "stim_dir = \"data/nsddata_stimuli/stimuli/nsd/\"\n",
    "img_stim_file = stim_dir + \"nsdimagery_stimuli.pkl3\"\n",
    "ex_file = open(img_stim_file, 'rb')\n",
    "imagery_dict = pickle.load(ex_file)\n",
    "print(imagery_dict.keys())\n",
    "ex_file.close()\n",
    "exps = imagery_dict['exps']\n",
    "cues = imagery_dict['cues']\n",
    "image_map  = imagery_dict['image_map']\n",
    "image_data = imagery_dict['image_data']\n",
    "# for i, im in enumerate(image_data):\n",
    "#     image = Image.fromarray(im.transpose(1,2,0))\n",
    "#     image.save(\"data/nsddata_stimuli/stimuli/imagery_images/{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x, mean=None, stddev=None, return_stats=False):\n",
    "    if mean is not None:\n",
    "        m = mean\n",
    "    else:\n",
    "        m = np.mean(x, axis=0, keepdims=True)\n",
    "    if stddev is not None:\n",
    "        s = stddev\n",
    "    else:\n",
    "        s = np.std(x, axis=0, keepdims=True)\n",
    "    if return_stats:\n",
    "        return (x - m)/(s+1e-6), m, s\n",
    "    else:\n",
    "        return (x - m)/(s+1e-6)\n",
    "\n",
    "\n",
    "## EXAMPLE CONDITION AVERAGED RESPONSES\n",
    "def condition_average(data, cond):\n",
    "    idx, idx_count = np.unique(cond, return_counts=True)\n",
    "    idx_list = [cond==i for i in np.sort(idx)]\n",
    "    avg_data = np.zeros(shape=(len(idx),)+data.shape[1:], dtype=np.float32)\n",
    "    for i,m in enumerate(idx_list):\n",
    "        avg_data[i] = np.mean(data[m], axis=0)\n",
    "    return avg_data\n",
    "\n",
    "meta_cond_idx = {\n",
    "    'visA': np.arange(len(exps))[exps=='visA'],\n",
    "    'visB': np.arange(len(exps))[exps=='visB'],\n",
    "    'imgA_1': np.arange(len(exps))[exps=='imgA_1'],\n",
    "    'imgA_2': np.arange(len(exps))[exps=='imgA_2'],\n",
    "    'imgB_1': np.arange(len(exps))[exps=='imgB_1'],\n",
    "    'imgB_2': np.arange(len(exps))[exps=='imgB_2']\n",
    "}\n",
    "\n",
    "cond_idx = {\n",
    "    'visA': np.arange(len(exps))[exps=='visA'],\n",
    "    'visB': np.arange(len(exps))[exps=='visB'],\n",
    "    'imgA': np.arange(len(exps))[np.logical_or(exps=='imgA_1', exps=='imgA_2')],\n",
    "    'imgB': np.arange(len(exps))[np.logical_or(exps=='imgB_1', exps=='imgB_2')]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE DATA WITHOUT GLOBAL ZSCORING (EVEN IF IT IS ZSCORED WE ARE GONNA RE-ZSCORE LATER)\n",
    "\n",
    "# voxel_data_raw = {}\n",
    "# subjects = [1,2,3,4,5,6,7,8]\n",
    "# for k,s in enumerate(subjects):\n",
    "#     # print ('--------  subject %d  -------' % s)\n",
    "#     create_whole_region_imagery_unnormalized(s)\n",
    "#     create_whole_region_imagery_normalized(s)\n",
    "    # voxel_data_raw[s] = torch.load(\"data/preprocessed_data/subject{}/nsd_imagery_unnormalized.pt\".format(s))\n",
    "    # print (voxel_data_raw[s].shape)\n",
    "\n",
    "\n",
    "# ## NORMALIZATION OF THE DATA FOR EACH INDIVIDUAL TRIAL\n",
    "# voxel_data_n = {}\n",
    "# for s in voxel_data_raw.keys():\n",
    "#     voxel_data_n[s] = np.copy(voxel_data_raw[s])\n",
    "#     for c,idx in meta_cond_idx.items():\n",
    "#         voxel_data_n[s][idx] = zscore(voxel_data_raw[s][idx])\n",
    "#     print(voxel_data_n[s])\n",
    "\n",
    "\n",
    "# cond_im_idx = {n: [image_map[c] for c in cues[idx]] for n,idx in cond_idx.items()}\n",
    "# ## EXAMPLE USE\n",
    "# for c, idx, im_idx in zip_dict(cond_idx, cond_im_idx): # loop conditions\n",
    "#     data_single = voxel_data_n[s][idx]\n",
    "#     data = condition_average(data_single, im_idx)\n",
    "#     print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 15724]) torch.Size([12, 1024])\n",
      "torch.Size([6, 15724]) torch.Size([6, 1024])\n",
      "torch.Size([6, 15724]) torch.Size([6, 1024])\n"
     ]
    }
   ],
   "source": [
    "x, y = load_nsd_mental_imagery(vector = \"c\", subject=1, mode=\"imagery\", stimtype=\"all\", average=True, nest=False)\n",
    "x, y = load_nsd_mental_imagery(vector = \"images\", subject=1, mode=\"imagery\", stimtype=\"simple\", average=True, nest=False)\n",
    "x, y = load_nsd_mental_imagery(vector = \"z_vdvae\", subject=1, mode=\"imagery\", stimtype=\"complex\", average=True, nest=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
