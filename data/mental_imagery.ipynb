{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torchvision.transforms as T\n",
    "from data_utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import PIL\n",
    "os.chdir(\"..\")\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw scanning session data:   0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw scanning session data: 100%|██████████| 37/37 [22:12<00:00, 36.00s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 72.3 GiB for an array with shape (2997, 6474000) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/naxos2-raid25/kneel027/home/kneel027/Second-Sight/data/mental_imagery.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhypatia.cmrr.umn.edu/home/naxos2-raid25/kneel027/home/kneel027/Second-Sight/data/mental_imagery.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m create_whole_region_unnormalized_sanity(\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/export/raid1/home/kneel027/Second-Sight/data/data_utils.py:353\u001b[0m, in \u001b[0;36mcreate_whole_region_unnormalized_sanity\u001b[0;34m(subject)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m         betas_all \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((betas_all,beta_trial),\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 353\u001b[0m whole_region \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(betas_all\u001b[39m.\u001b[39;49mreshape((\u001b[39mlen\u001b[39;49m(betas_all), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m    354\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/preprocessed_data/subject\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/nsd_general_unnormalized_unmasked.pt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject)\n\u001b[1;32m    355\u001b[0m \u001b[39m# Save the tensor into the data directory. \u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 72.3 GiB for an array with shape (2997, 6474000) and data type float32"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in [1,2,5,7]:\n",
    "    create_whole_region_imagery_unnormalized(subject, mask=False)\n",
    "    # create_whole_region_imagery_normalized(subject, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['meta_dm', 'gdm', 'gcm', 'image_data', 'image_map', 'exps', 'cues', 'modes'])\n"
     ]
    }
   ],
   "source": [
    "# nsd_root = '/export/raid1/home/surly/raid1/kendrick-data/nsd/'\n",
    "# stim_dir = nsd_root + 'nsddata_stimuli/stimuli/nsd/'\n",
    "# beta_dir = nsd_root + 'nsddata_betas/ppdata/'\n",
    "# mask_dir= nsd_root + 'nsddata/ppdata/'\n",
    "# img_stim_file = stim_dir + \"nsdimagery_stimuli.pkl3\"\n",
    "# beta_subj = beta_dir + \"subj%02d/func1pt8mm/nsdimagerybetas_fithrf/betas_nsdimagery.nii.gz\"%subj\n",
    "\n",
    "def image_feature_fn(image):\n",
    "    '''take uint8 image and return floating point (0,1), either color or bw'''\n",
    "    return image.astype(np.float32) / 255\n",
    "\n",
    "## LOAD THE STIM IMAGES AND SEQUENCE ALIGNMENT DESCRIPTORS\n",
    "stim_dir = \"data/nsddata_stimuli/stimuli/nsd/\"\n",
    "img_stim_file = stim_dir + \"nsdimagery_stimuli.pkl3\"\n",
    "ex_file = open(img_stim_file, 'rb')\n",
    "imagery_dict = pickle.load(ex_file)\n",
    "print(imagery_dict.keys())\n",
    "ex_file.close()\n",
    "exps = imagery_dict['exps']\n",
    "cues = imagery_dict['cues']\n",
    "image_map  = imagery_dict['image_map']\n",
    "image_data = imagery_dict['image_data']\n",
    "# for i, im in enumerate(image_data):\n",
    "#     image = Image.fromarray(im.transpose(1,2,0))\n",
    "#     image.save(\"data/nsddata_stimuli/stimuli/imagery_images/{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x, mean=None, stddev=None, return_stats=False):\n",
    "    if mean is not None:\n",
    "        m = mean\n",
    "    else:\n",
    "        m = np.mean(x, axis=0, keepdims=True)\n",
    "    if stddev is not None:\n",
    "        s = stddev\n",
    "    else:\n",
    "        s = np.std(x, axis=0, keepdims=True)\n",
    "    if return_stats:\n",
    "        return (x - m)/(s+1e-6), m, s\n",
    "    else:\n",
    "        return (x - m)/(s+1e-6)\n",
    "\n",
    "\n",
    "## EXAMPLE CONDITION AVERAGED RESPONSES\n",
    "def condition_average(data, cond):\n",
    "    idx, idx_count = np.unique(cond, return_counts=True)\n",
    "    idx_list = [cond==i for i in np.sort(idx)]\n",
    "    avg_data = np.zeros(shape=(len(idx),)+data.shape[1:], dtype=np.float32)\n",
    "    for i,m in enumerate(idx_list):\n",
    "        avg_data[i] = np.mean(data[m], axis=0)\n",
    "    return avg_data\n",
    "\n",
    "meta_cond_idx = {\n",
    "    'visA': np.arange(len(exps))[exps=='visA'],\n",
    "    'visB': np.arange(len(exps))[exps=='visB'],\n",
    "    'imgA_1': np.arange(len(exps))[exps=='imgA_1'],\n",
    "    'imgA_2': np.arange(len(exps))[exps=='imgA_2'],\n",
    "    'imgB_1': np.arange(len(exps))[exps=='imgB_1'],\n",
    "    'imgB_2': np.arange(len(exps))[exps=='imgB_2']\n",
    "}\n",
    "\n",
    "cond_idx = {\n",
    "    'visA': np.arange(len(exps))[exps=='visA'],\n",
    "    'visB': np.arange(len(exps))[exps=='visB'],\n",
    "    'imgA': np.arange(len(exps))[np.logical_or(exps=='imgA_1', exps=='imgA_2')],\n",
    "    'imgB': np.arange(len(exps))[np.logical_or(exps=='imgB_1', exps=='imgB_2')]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE DATA WITHOUT GLOBAL ZSCORING (EVEN IF IT IS ZSCORED WE ARE GONNA RE-ZSCORE LATER)\n",
    "\n",
    "# voxel_data_raw = {}\n",
    "# subjects = [1,2,3,4,5,6,7,8]\n",
    "# for k,s in enumerate(subjects):\n",
    "#     # print ('--------  subject %d  -------' % s)\n",
    "#     create_whole_region_imagery_unnormalized(s)\n",
    "#     create_whole_region_imagery_normalized(s)\n",
    "    # voxel_data_raw[s] = torch.load(\"data/preprocessed_data/subject{}/nsd_imagery_unnormalized.pt\".format(s))\n",
    "    # print (voxel_data_raw[s].shape)\n",
    "\n",
    "\n",
    "# ## NORMALIZATION OF THE DATA FOR EACH INDIVIDUAL TRIAL\n",
    "# voxel_data_n = {}\n",
    "# for s in voxel_data_raw.keys():\n",
    "#     voxel_data_n[s] = np.copy(voxel_data_raw[s])\n",
    "#     for c,idx in meta_cond_idx.items():\n",
    "#         voxel_data_n[s][idx] = zscore(voxel_data_raw[s][idx])\n",
    "#     print(voxel_data_n[s])\n",
    "\n",
    "\n",
    "# cond_im_idx = {n: [image_map[c] for c in cues[idx]] for n,idx in cond_idx.items()}\n",
    "# ## EXAMPLE USE\n",
    "# for c, idx, im_idx in zip_dict(cond_idx, cond_im_idx): # loop conditions\n",
    "#     data_single = voxel_data_n[s][idx]\n",
    "#     data = condition_average(data_single, im_idx)\n",
    "#     print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 15724]) torch.Size([12, 1024])\n",
      "torch.Size([6, 15724]) torch.Size([6, 1024])\n",
      "torch.Size([6, 15724]) torch.Size([6, 1024])\n"
     ]
    }
   ],
   "source": [
    "x, y = load_nsd_mental_imagery(vector = \"c\", subject=1, mode=\"imagery\", stimtype=\"all\", average=True, nest=False)\n",
    "x, y = load_nsd_mental_imagery(vector = \"images\", subject=1, mode=\"imagery\", stimtype=\"simple\", average=True, nest=False)\n",
    "x, y = load_nsd_mental_imagery(vector = \"z_vdvae\", subject=1, mode=\"imagery\", stimtype=\"complex\", average=True, nest=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
