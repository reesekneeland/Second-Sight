{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/raid1/home/kneel027/miniconda3/envs/SS/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/export/raid1/home/kneel027/miniconda3/envs/SS/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/export/raid1/home/kneel027/miniconda3/envs/SS/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys, shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from PIL import Image\n",
    "sys.path.append(\"../src\")\n",
    "from utils import *\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from transformers import AutoProcessor, AutoTokenizer, CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n",
    "os.chdir(\"../\")\n",
    "from reconstructor import Reconstructor\n",
    "# from reconstructor_bd import Reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################\n",
      "# Running in eps mode #\n",
      "#######################\n",
      "\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Load pth from pretrained/kl-f8.pth\n",
      "Load autoencoderkl with total 83653863 parameters,72921.759 parameter sum.\n",
      "Load optimus_bert_connector with total 109489920 parameters,19108.919 parameter sum.\n",
      "Load optimus_gpt2_connector with total 132109824 parameters,19220.966 parameter sum.\n",
      "Load pth from pretrained/optimus-vae.pth\n",
      "Load optimus_vae_next with total 241599744 parameters,-344611.688 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load clip_image_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load clip_text_context_encoder with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load openai_unet_2d_next with total 859520964 parameters,100402.990 parameter sum.\n",
      "Load openai_unet_0d_next with total 1706797888 parameters,250096.403 parameter sum.\n",
      "Load vd_v2_0 with total 3746805485 parameters,206824.483 parameter sum.\n",
      "Reconstructor initialized\n"
     ]
    }
   ],
   "source": [
    "# R = Reconstructor(fp16=False, device=\"cuda:2\")\n",
    "R = Reconstructor(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.83it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 231.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 271.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 259.82it/s]\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/naxos2-raid25/kneel027/home/kneel027/Second-Sight/output/mental_imagery_paper/\"\n",
    "version = \"2.3\"\n",
    "for mode in [\"vision\", \"imagery\"]:\n",
    "    for subject in [1, 2, 5, 7]:\n",
    "        for sample in tqdm(range(0, 1)):\n",
    "            sample_path = base_path + f\"{mode}/brain-optimized-inference-v{version}/subject{subject}/{sample}/\"\n",
    "            image_path = sample_path + f\"best_distribution/images/\"\n",
    "            for rep in range(10):\n",
    "                if os.path.exists(image_path + f\"{rep}.png\"):\n",
    "                    shutil.copy(image_path + f\"{rep}.png\", sample_path + f\"{rep}.png\")\n",
    "                else:\n",
    "                    if version == \"2.3\":\n",
    "                        clip_vis = torch.load(sample_path + \"clip_vision.pt\", map_location=\"cpu\")\n",
    "                        clip_text = torch.load(sample_path + \"clip_text.pt\", map_location=\"cpu\")\n",
    "                        textstrength=0.4\n",
    "                    elif version == \"2.1\":\n",
    "                        clip_vis = torch.load(sample_path + \"clip_best.pt\", map_location=\"cpu\")\n",
    "                        clip_text = None\n",
    "                        textstrength=0.0\n",
    "                    z_img = Image.open(sample_path + \"best_distribution/z_img.png\")\n",
    "                    with open(sample_path + \"best_distribution/strength.txt\", \"r\") as file:\n",
    "                        strength = float(file.read())\n",
    "                    new_img = R.reconstruct(image=z_img,\n",
    "                                            c_i=clip_vis,\n",
    "                                            c_t=clip_text,\n",
    "                                            strength=0.99,\n",
    "                                            textstrength=textstrength)\n",
    "                    new_img.save(sample_path + f\"{rep}.png\")\n",
    "                    new_img.save(image_path + f\"{rep}.png\")\n",
    "                    # print(sample_path + f\"{rep}.png\")\n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
